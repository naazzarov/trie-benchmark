\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

\lstset{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\bfseries\color{blue!70!black},
    commentstyle=\itshape\color{green!50!black},
    stringstyle=\color{orange!80!black},
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    backgroundcolor=\color{gray!5},
    tabsize=4,
    showstringspaces=false
}

\captionsetup{font=small, labelfont=bf}

\title{\textbf{Space-Time Trade-offs in Trie Variants\\for Large-Scale String Indexing in C++}}
\author{
    Bahodir Nazarov\\[0.5em]
    \textit{Independent Research Project}\\[0.3em]
    \small Department of Computer Science\\
    \small \texttt{bahodir.nazarov@email.com}
}
\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
\noindent
This paper explores the space-time trade-offs between three trie data structure variants: standard trie, compressed trie (radix tree), and double-array trie. I implemented all three in C++17 and ran benchmarks on datasets ranging from 1,000 to 370,000 words. The results show that compressed tries reduce memory usage by about 67\% compared to standard tries while keeping operations fast. Double-array tries achieve up to 94\% memory reduction but have significantly slower construction times. This research provides practical insights for developers choosing between trie implementations based on their specific needs.
\end{abstract}

\vspace{0.5em}
\noindent\textbf{Keywords:} Trie, Radix Tree, Double-Array Trie, Data Structures, C++, Benchmarking

\onehalfspacing

\section{Introduction}

String searching and storage is something we encounter everywhere in computing. From autocomplete in search engines to spell checkers in text editors, efficient string handling is crucial. During my studies, I became interested in how different data structures handle this problem, which led me to tries.

A trie (pronounced "try") is a tree-like structure where each node represents a character. The path from root to any node spells out a string prefix. What makes tries interesting is that lookup time depends only on the length of the string, not on how many strings are stored.

However, tries have a well-known problem: they use a lot of memory. Each node needs to store references to its children, and with large alphabets or many words, this adds up quickly. This got me wondering: are there ways to make tries more memory-efficient?

That question led to this project. I decided to implement three different trie variants and measure their performance:

\begin{enumerate}
    \item \textbf{Standard Trie} -- the basic implementation using hash maps for children
    \item \textbf{Compressed Trie} -- also called a radix tree, which merges single-child chains
    \item \textbf{Double-Array Trie} -- a clever technique using two arrays instead of pointers
\end{enumerate}

My goal was to answer these questions:
\begin{itemize}
    \item How much memory does each variant actually use?
    \item How fast are insertions and lookups?
    \item Which variant works best for different use cases?
\end{itemize}

\section{Background}

\subsection{What is a Trie?}

A trie is a tree structure for storing strings. Unlike a binary search tree where each node holds a complete key, a trie distributes the key across the path from root to leaf. For example, storing "cat" and "car" would share the path c $\rightarrow$ a, then branch to 't' and 'r'.

The name comes from "retrieval" and was introduced by Edward Fredkin in 1960 \cite{fredkin1960}. Tries are used in many real-world applications:

\begin{itemize}
    \item Autocomplete systems
    \item Spell checkers
    \item IP routing tables
    \item Dictionary implementations
\end{itemize}

\subsection{The Memory Problem}

The main drawback of standard tries is memory consumption. If we use an array of size 26 (for lowercase letters) at each node, most slots will be empty. Using a hash map helps, but still has overhead per node.

Consider storing 100,000 English words. A standard trie might create hundreds of thousands of nodes, each with its own memory overhead. This motivated researchers to develop more compact variants.

\subsection{Related Work}

Several solutions have been proposed over the years:

\textbf{Patricia Tries} were introduced by Morrison in 1968 \cite{morrison1968}. The idea is to skip over nodes that have only one child, storing the skipped characters on the edges. This is also called a "radix tree" or "compressed trie."

\textbf{Double-Array Tries} were proposed by Aoe in 1989 \cite{aoe1989}. Instead of using pointers, this approach stores the entire trie in two integer arrays. The technique is clever but makes insertion more complex.

Other approaches include Aho-Corasick automata for pattern matching \cite{aho1975} and suffix trees for more advanced string operations \cite{ukkonen1995}. For this project, I focused on the three variants mentioned above since they represent different points on the space-time trade-off spectrum.

\section{Implementation}

I implemented all three variants in C++17. The code is available on GitHub at \url{https://github.com/naazzarov/trie-benchmark}. Here I'll explain the key design decisions for each.

\subsection{Standard Trie}

For the standard trie, I used \texttt{std::unordered\_map} to store children at each node. This avoids wasting space on unused alphabet positions while still giving O(1) average-case child lookup.

\begin{lstlisting}
struct TrieNode {
    unordered_map<char, unique_ptr<TrieNode>> children;
    bool isEndOfWord = false;
};
\end{lstlisting}

The insert operation walks down the trie, creating nodes as needed:

\begin{lstlisting}
void insert(const string& word) {
    TrieNode* current = root.get();
    for (char c : word) {
        if (current->children.find(c) == current->children.end()) {
            current->children[c] = make_unique<TrieNode>();
        }
        current = current->children[c].get();
    }
    current->isEndOfWord = true;
}
\end{lstlisting}

This is straightforward but creates one node per character in the worst case.

\subsection{Compressed Trie (Radix Tree)}

The compressed trie adds an edge label to each node. Instead of one character per edge, we can have entire strings:

\begin{lstlisting}
struct TrieNode {
    unordered_map<char, unique_ptr<TrieNode>> children;
    string edgeLabel;
    bool isEndOfWord = false;
};
\end{lstlisting}

The tricky part is handling partial matches during insertion. If the new word shares a prefix with an existing edge but then diverges, we need to split the node. For example, inserting "test" when "testing" already exists requires splitting at position 4.

This adds complexity to the code, but the payoff is fewer nodes overall.

\subsection{Double-Array Trie}

The double-array trie is the most complex to implement. It uses two arrays:

\begin{lstlisting}
vector<int> base;   // base values for each state
vector<int> check;  // validation array
\end{lstlisting}

To check if there's a transition from state $s$ on character $c$:
\begin{enumerate}
    \item Calculate target state: $t = \text{base}[s] + \text{code}(c)$
    \item Verify the transition: check if $\text{check}[t] == s$
\end{enumerate}

The challenge is finding base values that don't cause collisions. When inserting a new word, we might need to relocate existing states to make room. This makes insertion slow, but lookups are very fast since we're just doing array indexing.

\subsection{Memory Calculation}

To measure memory usage, I implemented a \texttt{getMemoryUsage()} method for each variant. For the pointer-based tries, I recursively sum up the size of each node. For the double-array trie, it's simply the size of the two vectors.

\section{Experimental Setup}

\subsection{Test Environment}

I ran all benchmarks on my MacBook with the following setup:
\begin{itemize}
    \item macOS operating system
    \item Apple Clang compiler with C++17 standard
    \item Optimization flag: -O2
    \item Timing measured with \texttt{std::chrono::high\_resolution\_clock}
\end{itemize}

\subsection{Datasets}

I used four datasets of increasing size:

\begin{enumerate}
    \item \textbf{1K Random} -- 1,000 randomly generated strings (5-15 characters)
    \item \textbf{10K Random} -- 10,000 randomly generated strings
    \item \textbf{50K Random} -- 50,000 randomly generated strings
    \item \textbf{Real Dictionary} -- 370,105 English words from an open-source dictionary
\end{enumerate}

The random strings help test scalability, while the real dictionary shows performance on actual word patterns.

\subsection{Metrics}

For each variant and dataset, I measured:
\begin{itemize}
    \item \textbf{Memory Usage} -- total bytes used by the data structure
    \item \textbf{Insert Time} -- time to insert all words
    \item \textbf{Search Time} -- time to search for all words
    \item \textbf{Bytes per Word} -- memory divided by word count (for easy comparison)
\end{itemize}

Each benchmark was run multiple times to ensure consistent results.

\section{Results}

\subsection{Performance Tables}

\begin{table}[H]
\centering
\caption{Results on 1,000 Random Words}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Variant} & \textbf{Memory (KB)} & \textbf{Insert (ms)} & \textbf{Search (ms)} & \textbf{Bytes/Word} \\
\midrule
Standard Trie & 470 & 0.78 & 0.10 & 481 \\
Compressed Trie & 135 & 0.21 & 0.11 & 138 \\
Double-Array Trie & 88 & 31.91 & 0.00 & 90 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Results on 10,000 Random Words}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Variant} & \textbf{Memory (KB)} & \textbf{Insert (ms)} & \textbf{Search (ms)} & \textbf{Bytes/Word} \\
\midrule
Standard Trie & 4,360 & 7.08 & 0.16 & 446 \\
Compressed Trie & 1,280 & 1.80 & 0.12 & 131 \\
Double-Array Trie & 229 & 1,204 & 0.00 & 23 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Results on 50,000 Random Words}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Variant} & \textbf{Memory (KB)} & \textbf{Insert (ms)} & \textbf{Search (ms)} & \textbf{Bytes/Word} \\
\midrule
Standard Trie & 20,254 & 31.27 & 0.24 & 415 \\
Compressed Trie & 6,712 & 19.96 & 0.39 & 137 \\
Double-Array Trie & 1,144 & 30,160 & 0.01 & 23 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Results on 370,105 Dictionary Words}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Variant} & \textbf{Memory (KB)} & \textbf{Insert (ms)} & \textbf{Search (ms)} & \textbf{Bytes/Word} \\
\midrule
Standard Trie & 57,212 & 86.29 & 2.14 & 158 \\
Compressed Trie & 48,086 & 68.29 & 1.06 & 133 \\
Double-Array Trie & 4,150 & 217,110 & 0.03 & 11 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Visual Comparison}

Figure \ref{fig:memory_comparison} shows the memory usage across all dataset sizes. The difference between variants becomes more dramatic as the dataset grows.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/fig1_memory_comparison.pdf}
    \caption{Memory usage comparison across different dataset sizes. The Double-Array Trie consistently uses the least memory, while the Standard Trie uses the most.}
    \label{fig:memory_comparison}
\end{figure}

Figure \ref{fig:bytes_per_word} shows memory efficiency on the 50K word dataset. The Standard Trie uses 415 bytes per word, while the Double-Array Trie uses only 23 bytes -- a 94\% reduction.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig2_bytes_per_word.pdf}
    \caption{Memory efficiency comparison on 50,000 words. The annotations show the percentage reduction compared to the Standard Trie.}
    \label{fig:bytes_per_word}
\end{figure}

Figure \ref{fig:scalability} shows how memory usage scales with dataset size using a logarithmic x-axis. All three variants show roughly linear growth, but with very different slopes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/fig3_scalability.pdf}
    \caption{Memory scalability as dataset size increases. The gap between variants widens with larger datasets.}
    \label{fig:scalability}
\end{figure}

Figure \ref{fig:tradeoff} summarizes the key trade-off on the 370K word dictionary: the Double-Array Trie uses only 4 MB of memory but takes over 3 minutes to construct.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/fig4_tradeoff.pdf}
    \caption{Space-time trade-off on 370K words. Left: memory usage. Right: construction time (log scale). The Double-Array Trie offers the best memory efficiency but has significantly slower construction.}
    \label{fig:tradeoff}
\end{figure}

\subsection{Analysis}

\subsubsection{Memory Usage}

The results clearly show the memory differences between variants:

\begin{itemize}
    \item \textbf{Compressed Trie} uses about 67\% less memory than Standard Trie
    \item \textbf{Double-Array Trie} uses about 94\% less memory than Standard Trie
\end{itemize}

On the 370K word dictionary, the Standard Trie used 57 MB while the Double-Array Trie used only 4 MB. That's a huge difference for memory-constrained applications.

Interestingly, the bytes-per-word metric decreases as the dataset grows. This makes sense because larger datasets have more shared prefixes, so the overhead per word goes down.

\subsubsection{Insert Performance}

Insert times tell a different story:

\begin{itemize}
    \item \textbf{Compressed Trie} is actually faster than Standard Trie for insertion
    \item \textbf{Double-Array Trie} is much slower -- over 3.5 minutes for 370K words
\end{itemize}

The Double-Array Trie's slow insertion is expected. Finding valid base values and relocating colliding entries takes time. This variant is really meant for static datasets that are built once and queried many times.

\subsubsection{Search Performance}

All three variants have fast search times, but the Double-Array Trie has a slight edge:

\begin{itemize}
    \item All searches complete in under 3 milliseconds even for 370K words
    \item Double-Array Trie benefits from cache-friendly array access
\end{itemize}

The difference is small in absolute terms, but could matter for high-throughput applications.

\section{Discussion}

\subsection{When to Use Each Variant}

Based on my results, here's my recommendation:

\textbf{Use Standard Trie when:}
\begin{itemize}
    \item You have a small dataset (under 10K words)
    \item You need frequent insertions and deletions
    \item Simplicity is more important than memory efficiency
\end{itemize}

\textbf{Use Compressed Trie when:}
\begin{itemize}
    \item You want a good balance of memory and speed
    \item Your dataset is mostly static after initial loading
    \item You're building an autocomplete or prefix search system
\end{itemize}

\textbf{Use Double-Array Trie when:}
\begin{itemize}
    \item Memory is very limited
    \item You can build the trie offline (slow construction is acceptable)
    \item Search performance is critical
    \item Your dataset doesn't change
\end{itemize}

\subsection{Limitations}

This study has some limitations I should mention:

\begin{enumerate}
    \item I only tested English words and random ASCII strings. Results might differ for other alphabets or character sets.
    \item My Double-Array implementation could probably be optimized further. Production implementations use more sophisticated techniques.
    \item I didn't test deletion operations, which would be important for dynamic applications.
\end{enumerate}

\subsection{What I Learned}

Working on this project taught me several things:

\begin{enumerate}
    \item Data structure trade-offs are real and measurable. The "best" choice depends on your specific requirements.
    \item Implementation details matter. Using \texttt{unordered\_map} vs a fixed array has significant implications.
    \item Benchmarking is harder than it looks. Getting consistent measurements required careful setup.
\end{enumerate}

\section{Conclusion}

In this project, I implemented and benchmarked three trie variants to understand their space-time trade-offs. The results confirm what the literature suggests: you can save significant memory (up to 94\%) by using more sophisticated data structures, but there's usually a cost somewhere.

For most applications, I would recommend the Compressed Trie. It offers substantial memory savings (67\%) with no real performance penalty. The Double-Array Trie is great for read-heavy workloads where you can afford slow construction time.

Future work could explore:
\begin{itemize}
    \item Hybrid approaches that adapt based on subtree characteristics
    \item Concurrent/thread-safe implementations
    \item Serialization for persistent storage
    \item Comparison with other structures like hash tables or FSTs
\end{itemize}

The source code for this project is available at:\\
\url{https://github.com/naazzarov/trie-benchmark}

\section*{Acknowledgments}

I would like to thank my professors for their guidance and the open-source community for the dictionary dataset used in testing.

\begin{thebibliography}{9}

\bibitem{fredkin1960}
Fredkin, E. (1960). 
Trie memory. 
\textit{Communications of the ACM}, 3(9), 490--499.

\bibitem{morrison1968}
Morrison, D. R. (1968). 
PATRICIA -- Practical Algorithm to Retrieve Information Coded in Alphanumeric. 
\textit{Journal of the ACM}, 15(4), 514--534.

\bibitem{aho1975}
Aho, A. V., \& Corasick, M. J. (1975). 
Efficient string matching: An aid to bibliographic search. 
\textit{Communications of the ACM}, 18(6), 333--340.

\bibitem{aoe1989}
Aoe, J. (1989). 
An efficient digital search algorithm by using a double-array structure. 
\textit{IEEE Transactions on Software Engineering}, 15(9), 1066--1077.

\bibitem{ukkonen1995}
Ukkonen, E. (1995). 
On-line construction of suffix trees. 
\textit{Algorithmica}, 14(3), 249--260.

\bibitem{knuth1997}
Knuth, D. E. (1997). 
\textit{The Art of Computer Programming, Volume 3: Sorting and Searching} (2nd ed.). 
Addison-Wesley.

\bibitem{cormen2009}
Cormen, T. H., Leiserson, C. E., Rivest, R. L., \& Stein, C. (2009). 
\textit{Introduction to Algorithms} (3rd ed.). 
MIT Press.

\end{thebibliography}

\end{document}
